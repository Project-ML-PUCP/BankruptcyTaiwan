{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BankruptcyProject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M4gLpuREplp7"},"source":["## Importación de librerías"]},{"cell_type":"code","metadata":{"id":"9ecVSwJDWhU9","executionInfo":{"status":"ok","timestamp":1622828394535,"user_tz":300,"elapsed":707,"user":{"displayName":"Luis Miguel Enciso Salas","photoUrl":"","userId":"02884223281026621413"}}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import KFold\n","from scipy.io import arff\n","import random\n","from collections import OrderedDict\n","import seaborn as sns\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","import matplotlib.pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","from sklearn.metrics import confusion_matrix\n","from tabulate import tabulate\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JUGqjhlkpvmI"},"source":["## Lectura de modelos guardados"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cargamos los modelos\n","dt_model = joblib.load(os.path.join('modelos','Modelo_DT.pkl'))\n","lr_model = joblib.load(os.path.join('modelos','Modelo_LR.pkl'))\n","rf_model = joblib.load(os.path.join('modelos','Modelo_RF.pkl'))\n","xgb_model = joblib.load(os.path.join('modelos','Modelo_XGB.pkl'))\n","cv_svm_model = joblib.load(os.path.join('modelos','Modelo_SVM.pkl'))\n","cs_svm_ga_model = joblib.load(os.path.join('modelos','Modelo_GA_SVM.pkl'))"]},{"source":["## Lectura de dataset imputado"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# se carga dataset imputado\n","median_imputed_df = pd.read_pickle(os.path.join('results','median_imputed_data.pkl'))"]},{"cell_type":"markdown","metadata":{"id":"qVID3kM9p3mO"},"source":["## Evaluación de modelos"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Modelamiento de la data usando un diccionario de datasets y modelos\n","def perform_model_evaluation(_models_, imputed_df, verbose=False, k_folds=5):\n","    # 7 metricas, usando K-Folds, solo se evalua (no fit)\n","    # en model_results guardaremos los resultados por clasificador y datasets\n","    model_results = OrderedDict()\n","\n","    # Iteramos sobre los clasificadores\n","    for model_name, clf in _models_.items():\n","        if verbose: print(\"-\" * 120, \"\\n\", \"Model: \" + '\\033[1m' + model_name + '\\033[0m' + \" Classifier\")\n","        imputer_results = OrderedDict()\n","\n","        # hacemos la division del dataframe en variables y etiquetas\n","        features_df, labels_df = split_features_labels(imputed_df)\n","\n","        df_index = 0\n","        if verbose: print('\\t\\tDataset: ' + '\\033[1m' + str(df_index + 1) + 'year' + '\\033[0m')\n","        # Ejecutamos la validación cruzada K-fold en los sets de entranamiento y test\n","        X_train_list, y_train_list, X_test_list, y_test_list = kfold_cv(k_folds, features_df,\n","                                                                        labels_df, verbose)\n","\n","        metrics = OrderedDict()\n","\n","        # incializamos las metricas a guardar\n","        accuracy_list = np.zeros([k_folds])\n","        precision_list = np.zeros([k_folds, 2])\n","        recall_list = np.zeros([k_folds, 2])\n","        true_negs = np.zeros([k_folds])\n","        false_pos = np.zeros([k_folds])\n","        false_negs = np.zeros([k_folds])\n","        true_pos = np.zeros([k_folds])\n","\n","        # Iteramos sobre los k-folds para el cálculo de las métricas\n","        for k in range(k_folds):\n","            X_train = X_train_list[k]\n","            y_train = y_train_list[k]\n","            X_test = X_test_list[k]\n","            y_test = y_test_list[k]\n","\n","            # y predicción en el set de test\n","            y_test_predicted = clf.predict(X_test)\n","            # presentamos la matriz de confusión\n","            print(confusion_matrix(y_test_predicted, y_test))\n","\n","            # guardamos accuracy y recall\n","            _accuracy_ = accuracy_score(y_test, y_test_predicted, normalize=True)\n","            accuracy_list[k] = _accuracy_\n","            _recalls_ = recall_score(y_test, y_test_predicted, average=None)\n","            recall_list[k] = _recalls_\n","\n","            # guardamos precision\n","            _precisions_ = precision_score(y_test, y_test_predicted, average=None)\n","            precision_list[k] = _precisions_\n","\n","            # calculamos la matriz de confusión\n","            _confusion_matrix_ = confusion_matrix(y_test, y_test_predicted)\n","            mlp_cm = confusion_matrix(y_test, y_test_predicted)\n","\n","            # guardamos demás valores: TN, FP, FN, TP\n","            true_negs[k] = _confusion_matrix_[0][0]\n","            false_pos[k] = _confusion_matrix_[0][1]\n","            false_negs[k] = _confusion_matrix_[1][0]\n","            true_pos[k] = _confusion_matrix_[1][1]\n","\n","        # Hacemos la media en el caso de más datasets\n","        metrics['Accuracy'] = np.mean(accuracy_list)\n","        metrics['Precisions'] = np.mean(precision_list, axis=0)\n","        metrics['Recalls'] = np.mean(recall_list, axis=0)\n","        metrics['TN'] = np.mean(true_negs)\n","        metrics['FP'] = np.mean(false_pos)\n","        metrics['FN'] = np.mean(false_negs)\n","        metrics['TP'] = np.mean(true_pos)\n","\n","        # presentamos algunos valores\n","        if verbose:\n","            print('\\t\\t\\tAccuracy:', metrics['Accuracy'])\n","            print('\\t\\t\\tPrecision:', metrics['Precisions'])\n","            print('\\t\\t\\tRecall:', metrics['Recalls'])\n","\n","        # guardamos en el diccionario\n","        model_results[model_name] = metrics\n","\n","    # presentamos la matriz de confusión en modo mapa de calor\n","    sns.heatmap(mlp_cm, annot=True,\n","                xticklabels=['Non Bankrupt', 'Bankrupt'],\n","                yticklabels=['Non Bankrupt', 'Bankrupt'])\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()\n","\n","    return model_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hacer ranking por cada métrica\n","def perform_model_ranking_acc(models, imputers, results):\n","    column_headers = ['-','Accuracy'] \n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['Accuracy'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_prec(models, imputers, results):\n","    column_headers = ['-','Precisions']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['Precisions'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_rec(models, imputers, results):\n","    column_headers = ['-','Recalls']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['Recalls'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_tn(models, imputers, results):\n","    column_headers = ['-','TN']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['TN'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_fp(models, imputers, results):\n","    column_headers = ['-','FP']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['FP'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_fn(models, imputers, results):\n","    column_headers = ['-','FN']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['FN'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df\n","\n","def perform_model_ranking_tp(models, imputers, results):\n","    column_headers = ['-','FP']\n","    rows = []\n","    for model_name, model_details in results.items():\n","        row = [model_name]\n","        row.append(model_details['TP'])\n","        rows.append(row)\n","    results_df = pd.DataFrame(data=rows, columns = column_headers)\n","    return results_df"]},{"source":["## Análisis comparativo"],"cell_type":"markdown","metadata":{}},{"source":["**Comparación**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"source":["**Tablas**"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"source":["## Prducción de gráficos"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}